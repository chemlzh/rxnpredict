# install packages (if necessary) and load them
if (!require("pacman")) install.packages("pacman")
pacman::p_load(gplots, ggplot2, GGally, caret, 
ModelMetrics, glmnet, gridExtra, randomForest, 
scales, arm, corrplot, neuralnet)

# set the working directory to the location of the rxnpredict folder
setwd("C:\\Workbench\\Computer Science\\Python Programming\\rxnpredict")


# ============================================================================
# Load descriptor and yield data and prepare data for modeling
# ============================================================================

# load output table generated by python script
output.table <- read.csv("R\\output_table.csv", header=TRUE)

# scale the descriptor data
output.scaled <- as.data.frame(scale(output.table))

# load user-created yield data (label reactions without yield data as NA)
yield.data <- as.numeric(unlist(read.csv("yields.csv", header=FALSE, stringsAsFactors=FALSE)))

# append the yield data to the output table
output.scaled$yield <- yield.data

# remove rows where yield=NA and columns containing at least one NA
output.scaled <- output.scaled[!(is.na(output.scaled$yield)), ]
output.scaled <- output.scaled[ , colSums(is.na(output.scaled)) == 0]


# ============================================================================
# Split data and train various machine learning models
# ============================================================================

# Split into training and test set (70/30)
set.seed(1084)
size <- round(0.70*nrow(output.scaled))
training <- sample(nrow(output.scaled), size=size, replace=FALSE)
training.scaled <- output.scaled[training, ]
test.scaled <- output.scaled[-training, ]

# 10-fold cross-validation
train_control <- trainControl(method="cv", number=10, savePredictions=TRUE)

# k-nearest neighbor (kNN)
set.seed(8915)
knnFit <- train(yield ~ ., data=training.scaled, trControl=train_control, method="knn")
saveRDS(knnFit, "user_rds\\knnFit.rds")

# support vector machine (SVM)
set.seed(8915)
svmFit <- train(yield ~ ., data=training.scaled, trControl=train_control, method="svmLinear")
saveRDS(svmFit, "user_rds\\svmFit.rds")

# Bayes generalized linear model (GLM)
set.seed(8915)
bayesglmFit <- train(yield ~ ., data=training.scaled, trControl=train_control, method="bayesglm")
saveRDS(bayesglmFit, "user_rds\\bayesglmFit.rds")

# linear model
set.seed(8915)
lmFit <- train(yield ~ ., data=training.scaled, trControl=train_control, method="lm")
saveRDS(lmFit, "user_rds\\lmFit.rds")

# neural network (neuralnet package)
set.seed(8915)
n <- names(training.scaled)
f <- as.formula(paste("yield ~", paste(n[!n %in% "yield"], collapse = " + ")))
nnetFit <- neuralnet(f, data=training.scaled, hidden=c(100), linear.output=TRUE, threshold=1, stepmax=1e7)
saveRDS(nnetFit, "user_rds\\nnetFit.rds")

# random forest model
set.seed(8915)
rfFit <- train(yield ~ ., data=training.scaled, trControl=train_control, method="rf", importance=TRUE)
saveRDS(rfFit, "user_rds\\rfFit.rds")

# uncomment to read in previously trained model (so you don't have to train it every time)
# rfFit <- readRDS("user_rds\\rfFit.rds")


# ============================================================================
# Calculate R^2 and RMSE using test set and generate calibration plot
# ============================================================================

# predict yields for test set
knn.pred <- predict(knnFit, test.scaled)
svm.pred <- predict(svmFit, test.scaled)
bayesglm.pred <- predict(bayesglmFit, test.scaled)
lm.pred <- predict(lmFit, test.scaled)
nnet.pred <- predict(nnetFit, test.scaled)
rf.pred <- predict(rfFit, test.scaled)

# calculate R^2 and RMSE of k-nearest neighbor model and display in console
knn.r2 <- cor(knn.pred, test.scaled$yield)
knn.rmse <- rmse(knn.pred, test.scaled$yield)
knn.ppv <- ppv(knn.pred, test.scaled$yield, cutoff = 0)
knn.recall <- recall(knn.pred, test.scaled$yield, cutoff = 0)
print("k-nearest neighbor model: ")
print(paste0("R Squared = ", knn.r2, ", RMSE = ", knn.rmse, ", precision rate = ", knn.ppv, ", recall rate = ", knn.recall))

# calculate R^2 and RMSE of support vector machine model and display in console
svm.r2 <- cor(svm.pred, test.scaled$yield)
svm.rmse <- rmse(svm.pred, test.scaled$yield)
svm.ppv <- ppv(svm.pred, test.scaled$yield, cutoff = 0)
svm.recall <- recall(svm.pred, test.scaled$yield, cutoff = 0)
print("support vector machine model: ")
print(paste0("R Squared = ", svm.r2, ", RMSE = ", svm.rmse, ", precision rate = ", svm.ppv, ", recall rate = ", svm.recall))

# calculate R^2 and RMSE of Bayes generalized linear model and display in console
bayesglm.r2 <- cor(bayesglm.pred, test.scaled$yield)
bayesglm.rmse <- rmse(bayesglm.pred, test.scaled$yield)
bayesglm.ppv <- ppv(bayesglm.pred, test.scaled$yield, cutoff = 0)
bayesglm.recall <- recall(bayesglm.pred, test.scaled$yield, cutoff = 0)
print("Bayes generalized linear model: ")
print(paste0("R Squared = ", bayesglm.r2, ", RMSE = ", bayesglm.rmse, ", precision rate = ", bayesglm.ppv, ", recall rate = ", bayesglm.recall))

# calculate R^2 and RMSE of linear model and display in console
lm.r2 <- cor(lm.pred, test.scaled$yield)
lm.rmse <- rmse(lm.pred, test.scaled$yield)
lm.ppv <- ppv(lm.pred, test.scaled$yield, cutoff = 0)
lm.recall <- recall(lm.pred, test.scaled$yield, cutoff = 0)
print("Bayes generalized linear model: ")
print(paste0("R Squared = ", lm.r2, ", RMSE = ", lm.rmse, ", precision rate = ", lm.ppv, ", recall rate = ", lm.recall))

# calculate R^2 and RMSE of neural network model with 100 nodes and display in console
nnet.r2 <- cor(nnet.pred, test.scaled$yield)
nnet.rmse <- rmse(nnet.pred, test.scaled$yield)
nnet.ppv <- ppv(nnet.pred, test.scaled$yield, cutoff = 0)
nnet.recall <- recall(nnet.pred, test.scaled$yield, cutoff = 0)
print("neural network model with 100 nodes: ")
print(paste0("R Squared = ", nnet.r2, ", RMSE = ", nnet.rmse, ", precision rate = ", nnet.ppv, ", recall rate = ", nnet.recall))

# calculate R^2 and RMSE of random forest model and display in console
rf.r2 <- cor(rf.pred, test.scaled$yield)
rf.rmse <- rmse(rf.pred, test.scaled$yield)
rf.ppv <- ppv(rf.pred, test.scaled$yield, cutoff = 0)
rf.recall <- recall(rf.pred, test.scaled$yield, cutoff = 0)
print("random forest model: ")
print(paste0("R Squared = ", rf.r2, ", RMSE = ", rf.rmse, ", precision rate = ", rf.ppv, ", recall rate = ", rf.recall))

# plot calibration plot (saves in R\user_plots)
df_knn <- data.frame(x = knn.pred,
                 y = test.scaled$yield)
p1_knn <- ggplot(df_knn, aes(x = x, y = y)) +
    geom_point(alpha = 0.4) + 
    scale_x_continuous(breaks = seq(0,100,25), lim=c(0, 100)) +
    labs(x='Predicted Yield', y='Observed Yield') +  
    geom_segment(aes(x=0,xend=100,y=0,yend=100), linetype="dashed") +
    geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\user_plots\\knn_calibration_plot.png", width=5, height=4)

df_svm <- data.frame(x = svm.pred,
                 y = test.scaled$yield)
p1_svm <- ggplot(df_svm, aes(x = x, y = y)) +
    geom_point(alpha = 0.4) + 
    scale_x_continuous(breaks = seq(0,100,25), lim=c(0, 100)) +
    labs(x='Predicted Yield', y='Observed Yield') +  
    geom_segment(aes(x=0,xend=100,y=0,yend=100), linetype="dashed") +
    geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\user_plots\\svm_calibration_plot.png", width=5, height=4)

df_bayesglm <- data.frame(x = bayesglm.pred,
                 y = test.scaled$yield)
p1_bayesglm <- ggplot(df_bayesglm, aes(x = x, y = y)) +
    geom_point(alpha = 0.4) + 
    scale_x_continuous(breaks = seq(0,100,25), lim=c(0, 100)) +
    labs(x='Predicted Yield', y='Observed Yield') +  
    geom_segment(aes(x=0,xend=100,y=0,yend=100), linetype="dashed") +
    geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\user_plots\\bayesglm_calibration_plot.png", width=5, height=4)

df_lm <- data.frame(x = lm.pred,
                 y = test.scaled$yield)
p1_lm <- ggplot(df_lm, aes(x = x, y = y)) +
    geom_point(alpha = 0.4) + 
    scale_x_continuous(breaks = seq(0,100,25), lim=c(0, 100)) +
    labs(x='Predicted Yield', y='Observed Yield') +  
    geom_segment(aes(x=0,xend=100,y=0,yend=100), linetype="dashed") +
    geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\user_plots\\lm_calibration_plot.png", width=5, height=4)

df_nnet <- data.frame(x = nnet.pred,
                 y = test.scaled$yield)
p1_nnet <- ggplot(df_nnet, aes(x = x, y = y)) +
    geom_point(alpha = 0.4) + 
    scale_x_continuous(breaks = seq(0,100,25), lim=c(0, 100)) +
    labs(x='Predicted Yield', y='Observed Yield') +  
    geom_segment(aes(x=0,xend=100,y=0,yend=100), linetype="dashed") +
    geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\user_plots\\nnet_calibration_plot.png", width=5, height=4)

df_rf <- data.frame(x = rf.pred,
                 y = test.scaled$yield)
p1_rf <- ggplot(df_rf, aes(x = x, y = y)) +
    geom_point(alpha = 0.4) + 
    scale_x_continuous(breaks = seq(0,100,25), lim=c(0, 100)) +
    labs(x='Predicted Yield', y='Observed Yield') +  
    geom_segment(aes(x=0,xend=100,y=0,yend=100), linetype="dashed") +
    geom_smooth(method="loess", se=FALSE)
ggsave(file="R\\user_plots\\rf_calibration_plot.png", width=5, height=4)


# ============================================================================
# Create variable importance plot
# ============================================================================

# read in variable importance from trained model
rf_imp <- importance(rfFit$finalModel)
rf.imp.df <- cbind(as.data.frame(rf_imp), names(rf_imp[, 1]))
colnames(rf.imp.df)[1] <- "IncMSE"
colnames(rf.imp.df)[3] <- "descriptor"

# for descriptor names, replace "_" with " " and "." with "*"
rf.imp.df$descriptor <- gsub("_", " ", rf.imp.df$descriptor)
rf.imp.df$descriptor <- gsub("[.]", "*", rf.imp.df$descriptor)

# capitalize descriptor names
simpleCap <- function(x) {
    s <- strsplit(x, " ")[[1]]
    paste(toupper(substring(s, 1, 1)), substring(s, 2),
          sep="", collapse=" ")
}
rf.imp.df$descriptor <- sapply(rf.imp.df$descriptor, simpleCap)

# plot variable importance (saves in R\user_plots)
# USER: change '10' on next line to modify minimum cutoff for IncMSE
p2_rf <- ggplot(rf.imp.df[rf.imp.df$IncMSE>10, ], aes(x=reorder(descriptor, IncMSE), y=IncMSE)) +
    geom_bar(stat="identity") +
    scale_y_continuous(labels = comma) +
    labs(x="", y="Increase in Mean Squared Error (%)") + 
    coord_flip()
ggsave(file="R\\user_plots\\rf_variable_importance.png", width=7, height=5)